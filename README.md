# semantic-drift-score

# SDS: Semantic Drift Score

SDS (Semantic Drift Score) is an open-source tool for quantifying **meaning loss during text compression**, such as summarization, by measuring the **embedding-based semantic difference** between an original text and its summary.

Built for clarity, extensibility, and liberation, SDS helps track, compare, and audit semantic preservation across compression strategies.

---

## ğŸ”§ Features
- Compute cosine-based **semantic drift scores (SDS)**
- Compare results across different embedding models (e.g. GTE, Stella)
- Run single or dual-model analyses
- Command-line interface for automation
- Designed for long context support (up to 8192 tokens with current GTE and Stella models, you can change embedding models as desired)

---

## ğŸ“ Project Structure

```text
sds/
â”œâ”€â”€ sds.py                # Single-model CLI script
â”œâ”€â”€ dual_sds.py           # Run SDS with two models side-by-side
â”œâ”€â”€ requirements.txt      # Dependencies (sentence-transformers, torch, etc)
â”œâ”€â”€ LICENSE               # GPL v3 License
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ original.txt      # Example longform input
â”‚   â””â”€â”€ summary.txt       # Example compressed output
â””â”€â”€ tests/
    â””â”€â”€ test_sds.py       # Simple test suite for core functionality
```

---

## ğŸš€ Quick Start

### 1. Install dependencies:
```bash
pip install -r requirements.txt
```

### 2. Run with a single model:
```bash
python sds.py --original examples/original.txt --summary examples/summary.txt
```

### 3. Run with dual models (e.g., Stella and GTE):
```bash
python dual_sds.py --original examples/original.txt --summary examples/summary.txt
```

---

## ğŸ“ What Is SDS?

Semantic Drift Score is defined as:

```python
SDS = 1 - cosine_similarity(embedding(original), embedding(summary))
```

This gives a **semantic difference score** from 0.0 (perfect retention) to ~1.0 (max drift).

Use cases include:
- **Compression quality tracking**
- **Summarization evaluation**
- **LLM memory audits**

---

## ğŸ“Š SDS Evaluation Summary

SDS (Semantic Drift Score) was benchmarked on 500 randomly sampled human summaries from the CNN/DailyMail dataset. Using both GTE and Stella embedding models, we evaluated its alignment with established metrics like BERTScore, ROUGE, and BLEU.

Key findings:
- âœ… **Strong inter-model agreement** between GTE and Stella SDS (r = 0.786)
- âœ… **Moderate inverse correlation** with BERTScore F1 (â€“0.48 to â€“0.56)
- âœ… **Low correlation with ROUGE/BLEU**, confirming SDS captures meaning, not just token overlap
- âœ… **Low SDS values (~0.13 avg)** on human summaries establish a baseline for â€œgoodâ€ semantic fidelity

ğŸ“„ See full writeup in [`docs/sds_summary_findings.md`](docs/sds_summary_findings.md)

---

## ğŸ”’ License

This project is licensed under the GNU General Public License v3.0 (GPLv3). 
All derivative works must also remain free and open.

---

## ğŸŒ± Contributing

Want to help calibrate SDS or improve model integration?
- Fork the repo
- Create a feature branch
- Submit a PR

Join the projectâ€™s liberation mission â€” SDS is part of a larger initiative to keep AI **transparent, inspectable, and free**.

---

## ğŸ§ª Future Additions
- Optional chunking/rolling-window support for longer texts
- Compression-aware LLM memory scoring

---

## ğŸ™ Acknowledgments
- Stella by NovaSearch
- GTE by Alibaba
- sentence-transformers by UKPLab

And to everyone committed to building transparent, explainable, anti-enclosure AI tools.

---

## ğŸ“« Contact
github.com/picollo7

---

**This is Fucking Bullshit-approved software.**

ğŸŒ Free knowledge or nothing.

---
